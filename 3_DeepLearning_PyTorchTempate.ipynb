{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51818bb-e2a0-48fc-8030-aa2a9beb1122",
   "metadata": {},
   "source": [
    "## Ex. 3: Deep Learning basics using PyTorch\n",
    "--------------------------------------------------------------------\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a03166aa-32f7-49d9-81d3-43977c11f4e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional # For activations functions\n",
    "import torch.optim as optim\n",
    "import torch as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ece592f9-60ac-4500-b32d-73e278a565ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearClassifier (nn.Module):\n",
    "    \n",
    "    # Step 1: Initialize NN\n",
    "    def __init__(self, lr, n_classes, input_dims):\n",
    "        super(LinearClassifier, self).__init__();\n",
    "        \n",
    "        # 1.1. Three input fully-connected (fc) layers\n",
    "        self.fc_1 = nn.Linear(*input_dims, 128)\n",
    "        self.fc_2 = nn.Linear(128, 256)\n",
    "        self.fc_3 = nn.Linear(256, n_classes)\n",
    "        \n",
    "        # 1.2. Optimizer. self.parameters() tells optimizer what we are optimizing\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "        # 1.3. Loss function: Alternative MSELoss(). huber-loss etc.\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # 1.4. Detect GPU and use if available\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # 1.5. Send entire nn to this device\n",
    "        self.to(self.device)\n",
    "    \n",
    "    # PyTorch handles back-prop for us, but feed-forward also. we must provide\n",
    "    # Step 2: Feed-forward algo.\n",
    "    def feedforward(self, data):\n",
    "        # 2.1. Activate each layer and feed-forward into next\n",
    "        layer_1 = functional.sigmoid(self.fc_1(data))\n",
    "        layer_2 = functional.sigmoid(self.fc_2(layer_1))\n",
    "        \n",
    "        # 2.2. Output of last layer is NOT activate.\n",
    "        # The cross-entropy loss function activates this and converts it \n",
    "        #   into probabilities that will add up to 1.0\n",
    "        layer_3 = self.fc_3(layer_2)\n",
    "        \n",
    "        return (layer_3)\n",
    "    \n",
    "    # Step 3: Learn (later for Q-learning)\n",
    "    def learn(self, data, labels):\n",
    "        # 3.1. Initialize gradients before begining learning loop to zero\n",
    "        #      Reduces chance of cross-chatter or gradients mis-behaving or leaking from one iteration to another\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # 3.2. Convert data and labels to tensors - so as to be compatible with PyTorch\n",
    "        data = T.tensor(data).to(self.device)\n",
    "        labels = T.tensor(labels).to(self.device)\n",
    "        \n",
    "        print(data)\n",
    "        print(labels)\n",
    "        \n",
    "        # 3.3. Get predictions, i.e. outputs\n",
    "        predictions = self.feedforward(data)\n",
    "        \n",
    "        # 3.4. Compute loss = error = cost\n",
    "        loss = self.loss(predictions, labels)\n",
    "        \n",
    "        # 3.5 Backward propogate\n",
    "        self.backward()\n",
    "        \n",
    "        # 3.6 Call optimizer to optimize\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39c13b02-6e11-412d-a8aa-a434d58371c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = LinearClassifier(0.001, 2, (4,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fa38f1f-ecb8-442f-acc1-04b27b25fc2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.07726621e-02, 2.52504430e-01],\n",
       "       [1.23155270e-01, 3.95220516e-01],\n",
       "       [7.54188863e-01, 5.97814853e-01],\n",
       "       [4.16636973e-01, 1.66059927e-04]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.random.rand(4,2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae2b161b-d6f1-48cd-8aeb-dfce76b36457",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels = [0,1,1,0]\n",
    "labels = np.random.randint(2, size=(4))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74348348-b438-484a-af0b-7d8e551a23ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.0773e-02, 2.5250e-01],\n",
      "        [1.2316e-01, 3.9522e-01],\n",
      "        [7.5419e-01, 5.9781e-01],\n",
      "        [4.1664e-01, 1.6606e-04]], dtype=torch.float64)\n",
      "tensor([1, 0, 1, 0], dtype=torch.int32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 52\u001b[0m, in \u001b[0;36mLinearClassifier.learn\u001b[1;34m(self, data, labels)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# 3.3. Get predictions, i.e. outputs\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeedforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 3.4. Compute loss = error = cost\u001b[39;00m\n\u001b[0;32m     55\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(predictions, labels)\n",
      "Cell \u001b[1;32mIn[30], line 28\u001b[0m, in \u001b[0;36mLinearClassifier.feedforward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeedforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# 2.1. Activate each layer and feed-forward into next\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     layer_1 \u001b[38;5;241m=\u001b[39m functional\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     29\u001b[0m     layer_2 \u001b[38;5;241m=\u001b[39m functional\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_2(layer_1))\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# 2.2. Output of last layer is NOT activate.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# The cross-entropy loss function activates this and converts it \u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m#   into probabilities that will add up to 1.0\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\rajeshs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\rajeshs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "l.learn(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5549850-33e5-46db-b6ee-753f8ea0e1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
